{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ETL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eMA1N12d-GT"
      },
      "source": [
        "# IMPORTING python libraries\n",
        "import numpy as np\n",
        "import imageio\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing as prep\n",
        "from google.colab import drive\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5azBPnlaKBFa"
      },
      "source": [
        "class ETL:\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    # This method uses Google Drive and zipped file. Please change the links as per your convenience\n",
        "\n",
        "    drive.mount('/content/drive')\n",
        "    !unzip -uq \"/content/drive/My Drive/Colab Notebooks/TopOPT/data/image.zip\"\n",
        "\n",
        "    self.img_path = \"data/\"\n",
        "    self.csv_path =  \"/content/drive/My Drive/Colab Notebooks/TopOPT/data/\"\n",
        "    self.gsize = (7,7)\n",
        "    self.IMG_SHAPE = (32,64)\n",
        "    self.no_of_data_points = 15000\n",
        "    self.csv_splits = [[1,1500],[1501,2200],[2201,2700],[2700,4500],[4501,5225],[5225,7000],[7001,8000],\n",
        "                       [8000,10000],[10000,11725],[11725,12500],[12500,15000]]\n",
        "\n",
        "\n",
        "  def initiate(self,augment = False):\n",
        "    \n",
        "    self.load_dataset()\n",
        "    self.clean_data()\n",
        "\n",
        "    if augment is True:\n",
        "      self.augment_data()\n",
        "      print('Augmented')\n",
        "    \n",
        "    self.data_norm()\n",
        "\n",
        "\n",
        "  # Load the dataset-----------------------\n",
        "  def load_dataset(self):\n",
        "    \n",
        "    # Loading Time and compliance data\n",
        "    self.load_comp_time() \n",
        "  \n",
        "    # Loading images as 6 channels\n",
        "    self.train_images = []\n",
        "\n",
        "    for i in tqdm(range(1,self.no_of_data_points+1)):\n",
        "      i_mod = \"{:01d}\".format(i)  \n",
        "      # try:   \n",
        "      self.train_images.append(self.load_image(i_mod))\n",
        "      #except:\n",
        "       # print('Not found {}', i) \n",
        "\n",
        "  # Cleaning DataSet---------------------------- \n",
        "  def clean_data(self):\n",
        "    faulty_cases = self.find_faulty() # FINDING FAULTY CASES\n",
        "    self.comp_time = np.delete(self.comp_time,faulty_cases,axis=0) # DELETING FAULTY CASES from COMP_TIME\n",
        "    self.comp_time = self.comp_time[:,[1,2]] # REMOVING EXTRA ROWS\n",
        "    self.train_images = np.delete(self.train_images,faulty_cases,0) # DELETING FAULTY CASES FROM TRAIN IMAGES\n",
        "\n",
        "  # Find Faulty Cases---------------------\n",
        "  def find_faulty(self):\n",
        "    data = self.comp_time\n",
        "    # Due to automatic generator for data, some generated cases resulted in an invalid solution, need to clean data\n",
        "\n",
        "    # 0 --> INDEX, 1--> Compliance, 2--> Compute Time\n",
        "    # Imaginary Numbers generated, additional rows generated 3,4,5\n",
        "    \n",
        "    imag_faulty = data[:,4]  # IMAGINARY NUMBER IN LAST ROWS\n",
        "    K1 = np.nonzero(imag_faulty>0)\n",
        "\n",
        "    comp_faulty = data[:,1] # COMPLIANCE TOO HIGH --> NOT OPTIMIZED BY THE SIMP CORRECTLY \n",
        "\n",
        "    cmf = comp_faulty[np.nonzero(comp_faulty>0)]\n",
        "    cmf = cmf[np.nonzero(cmf<5000)]\n",
        "    ts = np.mean(cmf) + 2*np.std(cmf)\n",
        "    K2 = np.nonzero(comp_faulty>ts) # Compliance is not in 2 std of mean  \n",
        "\n",
        "    return np.union1d(K1,K2)   # Return faulty cases\n",
        "    \n",
        "    # COMP TIME----------------------------------------\n",
        "  def load_comp_time(self):\n",
        "\n",
        "        for s in self.csv_splits:\n",
        "          a = s[0]\n",
        "          b = s[1]\n",
        "          print(a, b)\n",
        "          # try:\n",
        "          if a is 1:\n",
        "            self.comp_time = self.load_csv(a,b)\n",
        "          else:\n",
        "            self.comp_time = np.vstack((self.comp_time,self.load_csv(a,b)))\n",
        "          # except:\n",
        "          #    print('Not found {} - {}',a,b)\n",
        "        self.comp_time = np.array(self.comp_time)\n",
        "\n",
        "        \n",
        "  def load_csv(self,a,b):\n",
        "        a = \"{:01d}\".format(a) \n",
        "        b = \"{:01d}\".format(b) \n",
        "\n",
        "        path = self.csv_path + \"comp_time_\" + a + \"_\" + b + \"_.csv\"\n",
        "        CT = pd.read_csv(path, header= None)\n",
        "\n",
        "        # Complex to real     \n",
        "        comp_time = self.complex_to_real(CT,int(b))\n",
        "        comp_time = comp_time[int(a):int(b),:]\n",
        "        return comp_time\n",
        "        \n",
        "\n",
        "  def complex_to_real(self,data,ds):\n",
        "        # 0 --> INDEX + j compliance, 1--> compute_time + j X, 2--> Y + Z\n",
        "        # OUTPUT DATA --> 0--> index, 1-->compliance, 2-->time, 3-->0ss 4--->ERROR 5-->0s\n",
        "\n",
        "        comp_time = np.zeros((ds,6), dtype = float)\n",
        "\n",
        "        for i in range(ds):\n",
        "          for j in range(3):\n",
        "\n",
        "            a = data[j][i]  \n",
        "            real, imag = a.split('+') # SPLITTING COMPLEX to real\n",
        "            imag = imag[:-1] # Mag of imaginary number\n",
        "            real = float(real) # Float\n",
        "            imag = float(imag)\n",
        "            comp_time[i,2*j] = real\n",
        "            comp_time[i,2*j+1] = imag\n",
        "\n",
        "        return comp_time\n",
        "\n",
        "    # LOAD IMAGE--------------------------------------\n",
        "  def load_image(self,i_mod,blur=True):\n",
        "        path = self.img_path\n",
        "        \n",
        "        OUT = 255-self.image_read(path + \"OUT/\"+\"OUT_\"+ i_mod + \".png\") # 1 represents presence of material\n",
        "        BX = 255-self.image_read(path + \"B/\"+\"BFX_\"+ i_mod + \".png\") # 1 represents fixeddofs\n",
        "        BY  = 255-self.image_read(path + \"B/\"+\"BFY_\"+ i_mod + \".png\") # 1 represents fixeddofs\n",
        "        FX  = self.image_read(path + \"F/\"+\"FX_\"+ i_mod + \".png\") # 0.5 represents 0 force\n",
        "        FY = self.image_read(path + \"F/\"+\"FY_\"+ i_mod + \".png\")  # 0.5 represents 0 force \n",
        "        VF  = self.image_read(path + \"VF/\"+\"VF_\"+ i_mod+\".png\")\n",
        "\n",
        "        if blur == True:\n",
        "          BX  =  cv2.GaussianBlur(BX,self.gsize,cv2.BORDER_DEFAULT)\n",
        "          BY =  cv2.GaussianBlur(BY,self.gsize,cv2.BORDER_DEFAULT)\n",
        "          FX  =  cv2.GaussianBlur(FX,self.gsize,cv2.BORDER_DEFAULT)\n",
        "          FY =  cv2.GaussianBlur(FY,self.gsize,cv2.BORDER_DEFAULT)  \n",
        "        \n",
        "        # Default arrangement of channels\n",
        "        IMG = np.stack([OUT,BX,BY,FX,FY,VF],2)\n",
        "        return IMG\n",
        "\n",
        "\n",
        "  def image_read(self,dir,a = False):\n",
        "\n",
        "    if a is False:\n",
        "      img = cv2.imread(dir,0)\n",
        "    else:\n",
        "      img = cv2.imread(dir)\n",
        "\n",
        "    width = self.IMG_SHAPE[1]\n",
        "    height = self.IMG_SHAPE[0]\n",
        "    dim = (width, height)\n",
        "    resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
        "    resized =  np.asarray(resized).astype('float32')\n",
        "    return resized\n",
        "\n",
        "  # AUGMENTING DATA---------------------\n",
        "  def augment_data(self):\n",
        "      images = []\n",
        "      comp = []\n",
        "\n",
        "      for img,c in tqdm(zip(self.train_images,self.comp_time)):\n",
        "\n",
        "        flipped = np.zeros_like(img)\n",
        "        updown = np.zeros_like(img)\n",
        "        \n",
        "        for i in range(6):\n",
        "          s = img[:,:,i:i+1]\n",
        "          flipped[:,:,i:i+1] = tf.image.flip_left_right(s)\n",
        "          updown[:,:,i:i+1] = tf.image.flip_up_down(s) \n",
        "\n",
        "        images.append(img)  \n",
        "        comp.append(c)\n",
        "        images.append(flipped)\n",
        "        comp.append(c)\n",
        "        images.append(updown)\n",
        "        comp.append(c)\n",
        "\n",
        "      self.train_images = np.array(images).astype('float32')\n",
        "      self.comp_time = np.array(comp).astype('float32')  \n",
        "    \n",
        "  def data_norm(self):\n",
        "\n",
        "      self.train_images = np.array(self.train_images, dtype = float)\n",
        "      self.train_images = (self.train_images - 127.5)/127.5\n",
        "\n",
        "      # self.comp_time = np.array(self.comp_time, dtype = float)\n",
        "      # self.comp_time = prep\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1kDRq2WGgJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be743e4b-6948-45eb-b731-e589cc14913d"
      },
      "source": [
        "dataset = ETL()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ygF3iyXYvLF",
        "outputId": "c3e9dcaf-9c37-4df7-a48d-1783c649067e"
      },
      "source": [
        "dataset.initiate(augment=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 1500\n",
            "1501 2200\n",
            "2201 2700\n",
            "2700 4500\n",
            "4501 5225\n",
            "5225 7000\n",
            "7001 8000\n",
            "8000 10000\n",
            "10000 11725\n",
            "11725 12500\n",
            "12500 15000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15000/15000 [00:17<00:00, 854.62it/s]\n",
            "14116it [00:50, 280.44it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Augmented\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIEPyMXYZK3h",
        "outputId": "f9b0014a-31a7-44f5-b35d-77afeb5ebebc"
      },
      "source": [
        "dataset.train_images.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42348, 32, 64, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM2puOfadOqx",
        "outputId": "8040083a-3cc8-4a14-abb6-cbe887355e64"
      },
      "source": [
        "dataset.comp_time.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42348, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    }
  ]
}